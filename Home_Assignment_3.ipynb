{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xwCw1Nh_Wso"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1 Implementing an RNN for Text Generation\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# === Step 1: Load and preprocess data ===\n",
        "path_to_file = tf.keras.utils.get_file(\"shakespeare.txt\",\n",
        "    \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\")\n",
        "\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "print(f\"Length of text: {len(text)} characters\")\n",
        "\n",
        "# Character-level tokenization\n",
        "vocab = sorted(set(text))\n",
        "char2idx = {char: i for i, char in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "\n",
        "# Create sequences\n",
        "seq_length = 100\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\n",
        "\n",
        "def split_input_target(chunk):\n",
        "    return chunk[:-1], chunk[1:]\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "# Batch and shuffle\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "# === Step 2: Build the model ===\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024\n",
        "\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Input(batch_shape=(batch_size, None)),  # ✅ Keras 3 compatible\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
        "        tf.keras.layers.LSTM(\n",
        "            rnn_units,\n",
        "            return_sequences=True,\n",
        "            stateful=True,\n",
        "            recurrent_initializer='glorot_uniform'\n",
        "        ),\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "\n",
        "model = build_model(vocab_size, embedding_dim, rnn_units, BATCH_SIZE)\n",
        "\n",
        "# === Step 3: Compile and train ===\n",
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "# === Step 4: Checkpointing (Keras 3 fix: use .weights.h5) ===\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt.weights.h5\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True\n",
        ")\n",
        "\n",
        "# === Step 5: Train ===\n",
        "EPOCHS = 10\n",
        "model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hW7u8DraDmwq",
        "outputId": "ef133404-3fbd-405b-ad45-ac86bee8ee63"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 98939 characters\n",
            "Epoch 1/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 8s/step - loss: 3.7119\n",
            "Epoch 2/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 8s/step - loss: 3.0415\n",
            "Epoch 3/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 7s/step - loss: 2.6128\n",
            "Epoch 4/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 7s/step - loss: 2.3443\n",
            "Epoch 5/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 8s/step - loss: 2.1866\n",
            "Epoch 6/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 7s/step - loss: 2.0656\n",
            "Epoch 7/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 7s/step - loss: 1.9950\n",
            "Epoch 8/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 8s/step - loss: 1.9227\n",
            "Epoch 9/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 7s/step - loss: 1.8687\n",
            "Epoch 10/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 7s/step - loss: 1.8052\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7853a0b02750>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Step 6: Load model for generation (batch_size = 1) ===\n",
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "model.load_weights(checkpoint_prefix)  # ✅ Use full .weights.h5 path\n",
        "model.build(tf.TensorShape([1, None]))\n",
        "\n",
        "# === Step 7: Text generation function ===\n",
        "def generate_text(model, start_string, temperature=1.0, num_generate=1000):\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "    text_generated = []\n",
        "\n",
        "    # 🔄 Reset only the stateful RNN layers\n",
        "    for layer in model.layers:\n",
        "        if hasattr(layer, 'reset_states'):\n",
        "            layer.reset_states()\n",
        "\n",
        "    for _ in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "    return start_string + ''.join(text_generated)\n",
        "\n",
        "\n",
        "# === Step 8: Try generating text ===\n",
        "print(\"\\nGenerated text with temperature = 1.0:\\n\")\n",
        "print(generate_text(model, start_string=\"ROMEO:\", temperature=1.0))\n",
        "\n",
        "print(\"\\nGenerated text with temperature = 0.5:\\n\")\n",
        "print(generate_text(model, start_string=\"ROMEO:\", temperature=0.5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpVYcACJ6TEZ",
        "outputId": "1a255675-dc98-4f0c-e357-f66d5bae50f5"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated text with temperature = 1.0:\n",
            "\n",
            "ROMEO:\n",
            "\n",
            "The verir solm, ast glanth his and meys\n",
            "So  le leive the dost nome ture aruvenst fall farne steem fil ste till for of for thin the deev.\n",
            "\n",
            "Cokt des, be bome’s faot wolds perais:\n",
            "Sak ipie drencoun oftingece:\n",
            "But if eyes dear’d be the ottreis ger?\n",
            "And ane haring thou sweet re bynenery wall lovelaty be,\n",
            "The pance to llove my comE,\n",
            "Thy in suplancs mif to me:\n",
            "    Buk no ly dayy-or ferp-decutes,\n",
            "Than sheey and your lives with fae hast,\n",
            "Whin bes paze of baking shaye hin toum’dt exved thy trom sould.\n",
            "\n",
            "CV\n",
            "\n",
            "  And my ptorn my write endrencore exfion amon ey-be’cece;\n",
            "Thenet,\n",
            "Or bot thee and arawht my hmapk?\n",
            "And fornich exsuraines to of stiel,\n",
            "T  love y wiTh wall eyed dleave lokss the commalion.\n",
            "    WiYing be cam see,\n",
            "Theme or f it, you weethoun wind love steeps grisw\n",
            "Thin sull e sown the thon mower unceafse rave?\n",
            "Alven is sillfouss?\n",
            "What im delf tise youl of and d,\n",
            "   Huther cance’d swell rive bustong swrings;\n",
            "Wheve stroul’s doth withy sweether the mine, de thou cenjui\n",
            "\n",
            "Generated text with temperature = 0.5:\n",
            "\n",
            "ROMEO:\n",
            "\n",
            "CXII\n",
            "\n",
            "To mane the dand of my speis deald to mase,\n",
            "And the still the steet centrands love you and stame\n",
            "Thy love the beauty dour thy mender of fre pust to his and the preast a and the in the toul maye,\n",
            "And fair her to stee, love the sully and be thy still ane and mins my sweet,\n",
            "And thou mingt of in the love, breds in the sucther his the hear the hart the kessing the fremest and than will my heart,\n",
            "That and mane my dides be all be still,\n",
            "The haclon will whe calles and lind,\n",
            "And the if my love thou and heart,\n",
            "The senfell but love wher in thee hand bast the warte;\n",
            "That a leis and thy swenty thou show bearted with mese to thou lease.\n",
            "\n",
            "LXXII\n",
            "\n",
            "What a be a store thy the hame selm and beauty,\n",
            "And that thou all the seart, and my beauty and my see,\n",
            "My the storm thy seem hath thy gortes fair my lide,\n",
            "    And you thy she tore hermy in me pieating my from seet,\n",
            "The love thou love the not the pair to me will shee.\n",
            "\n",
            "CXXIII\n",
            "\n",
            "So mone prise the willd should and of all and heart\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Temperature Scaling Explanation\n",
        "Temperature scaling controls randomness in text generation:\n",
        "- High temperature (>1.0) introduces more randomness, making outputs creative but potentially incoherent.\n",
        "- Low temperature (<1.0) leads to deterministic predictions, producing repetitive and predictable text.\n",
        "- Moderate temperature (≈0.7) strikes a balance, allowing diverse yet meaningful sequences.\"\"\"\n"
      ],
      "metadata": {
        "id": "UBk_w2isXNR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2.NLP Preprocessing Pipeline\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "# Download the specific resource suggested by the error message\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "def nlp_preprocessing(sentence):\n",
        "    # Step 1: Tokenize the sentence\n",
        "    tokens = word_tokenize(sentence)\n",
        "\n",
        "    # Step 2: Remove stopwords\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    tokens_without_stopwords = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "    # Step 3: Apply stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_words = [stemmer.stem(word) for word in tokens_without_stopwords]\n",
        "\n",
        "    # Print outputs\n",
        "    print(\"Original Tokens:\", tokens)\n",
        "    print(\"Tokens Without Stopwords:\", tokens_without_stopwords)\n",
        "    print(\"Stemmed Words:\", stemmed_words)\n",
        "\n",
        "# Example sentence\n",
        "sentence = \"NLP techniques are used in virtual assistants like Alexa and Siri.\"\n",
        "nlp_preprocessing(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Za7Zpsmxufxc",
        "outputId": "9dabb54b-5d5f-4322-b814-74db820530a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Tokens: ['NLP', 'techniques', 'are', 'used', 'in', 'virtual', 'assistants', 'like', 'Alexa', 'and', 'Siri', '.']\n",
            "Tokens Without Stopwords: ['NLP', 'techniques', 'used', 'virtual', 'assistants', 'like', 'Alexa', 'Siri', '.']\n",
            "Stemmed Words: ['nlp', 'techniqu', 'use', 'virtual', 'assist', 'like', 'alexa', 'siri', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"### **1. Difference Between Stemming and Lemmatization**\n",
        "Both **stemming** and **lemmatization** reduce words to their root forms, but they do so differently:\n",
        "- **Stemming** is a **rule-based approach** that simply chops off word suffixes. It doesn’t always produce valid words.\n",
        "- **Lemmatization** uses **linguistic analysis** to find a word's **dictionary base form (lemma)**, ensuring proper meaning.\n",
        "\n",
        "#### **Example with \"Running\"**\n",
        "- **Stemming (Porter Stemmer)** → `\"running\"` → `\"run\"`\n",
        "- **Lemmatization (WordNet Lemmatizer)** → `\"running\"` → `\"run\"` (verb) / `\"running\"` (noun, unchanged)\n",
        "\n",
        "Lemmatization is **more accurate** because it considers the word's context, whereas stemming may create non-existent words like `\"play\" → \"playe\"`.\n",
        "\n",
        "### **2. Stop Word Removal: When It Helps vs. When It Harms**\n",
        "Advantages:\n",
        "- **Search engines** – Removing stopwords like `\"the\"`, `\"is\"` can improve indexing efficiency.\n",
        "- **Sentiment analysis** – Focuses on meaningful words like `\"excellent\"`, `\"terrible\"` rather than `\"the\"`, `\"and\"`.\n",
        "- **Topic modeling** – Helps extract **key themes** from text by eliminating common filler words.\n",
        "\n",
        "Disadvantages\n",
        "- **Legal or medical text processing** – Words like `\"not\"` or `\"no\"` are critical for **meaning**, e.g., `\"Patient has no allergies\"` vs. `\"Patient has allergies\"`.\n",
        "- **Machine translation** – Stopwords often impact **grammatical correctness**, making sentence structure incomplete.\n",
        "- **Question answering systems** – `\"What is the capital of France?\"` loses meaning if `\"What\"` is removed.\"\"\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rlY0OAUkxgeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3.Named Entity Recognition with SpaCy\n",
        "import spacy\n",
        "\n",
        "# Load the spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def extract_named_entities(text):\n",
        "    # Process the text with spaCy\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Print detected entities\n",
        "    for ent in doc.ents:\n",
        "        print(f\"Entity: {ent.text}, Label: {ent.label_}, Start: {ent.start_char}, End: {ent.end_char}\")\n",
        "\n",
        "# Example sentence\n",
        "sentence = \"Barack Obama served as the 44th President of the United States and won the Nobel Peace Prize in 2009.\"\n",
        "extract_named_entities(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5zaPFERut9l",
        "outputId": "7ddb15f9-9e78-41fb-de25-064478a0dd27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entity: Barack Obama, Label: PERSON, Start: 0, End: 12\n",
            "Entity: 44th, Label: ORDINAL, Start: 27, End: 31\n",
            "Entity: the United States, Label: GPE, Start: 45, End: 62\n",
            "Entity: the Nobel Peace Prize, Label: WORK_OF_ART, Start: 71, End: 92\n",
            "Entity: 2009, Label: DATE, Start: 96, End: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"### **1. Difference Between NER and POS Tagging**\n",
        "Named Entity Recognition (**NER**) and Part-of-Speech (**POS**) tagging are both NLP techniques, but they serve different purposes:\n",
        "- **NER** identifies and classifies named entities such as **persons, locations, organizations, dates, and monetary values** within text. Example: In *\"Microsoft was founded in 1975 by Bill Gates,\"* NER would detect **\"Microsoft\" (ORG), \"1975\" (DATE), \"Bill Gates\" (PERSON).**\n",
        "- **POS Tagging** assigns grammatical roles to words (e.g., **noun, verb, adjective, adverb**). Example: *\"The quick brown fox jumps over the lazy dog.\"* POS tagging would label **\"fox\" (noun), \"jumps\" (verb), \"quick\" (adjective).**\n",
        "\n",
        "Essentially, **NER focuses on meaning and identifying named entities, while POS tagging focuses on grammatical function.**\n",
        "\n",
        "### **2. Real-World Applications of NER**\n",
        " **Financial News & Sentiment Analysis:**\n",
        "Banks and financial analysts use **NER to detect company names, stock symbols, and dates** in news articles. For example, if an article states *\"Tesla’s stock rose by 5% on Tuesday,\"* NER can extract **\"Tesla\" (ORG), \"5%\" (PERCENT), \"Tuesday\" (DATE)** to predict market trends.\n",
        "\n",
        "**Search Engines & AI Chatbots:**\n",
        "Search engines use **NER to improve query understanding and deliver relevant results.** If a user searches *\"Restaurants in New York open past midnight,\"* NER helps categorize **\"New York\" (GPE - Geopolitical Entity), \"Restaurants\" (ORG), \"midnight\" (TIME)** so the search algorithm can refine location-based results.\"\"\"\n"
      ],
      "metadata": {
        "id": "7ooj8ONZxKbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4.Scaled Dot-Product Attention\n",
        "import numpy as np\n",
        "\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x))  # Stability trick for softmax\n",
        "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
        "\n",
        "def scaled_dot_product_attention(Q, K, V):\n",
        "    d_k = Q.shape[-1]  # Key dimension\n",
        "    attention_scores = np.dot(Q, K.T) / np.sqrt(d_k)  # Dot product and scaling\n",
        "\n",
        "    attention_weights = softmax(attention_scores)  # Softmax to get attention weights\n",
        "\n",
        "    output = np.dot(attention_weights, V)  # Weighted sum with Value matrix\n",
        "\n",
        "    return attention_weights, output\n",
        "\n",
        "# Test inputs\n",
        "Q = np.array([[1, 0, 1, 0], [0, 1, 0, 1]])\n",
        "K = np.array([[1, 0, 1, 0], [0, 1, 0, 1]])\n",
        "V = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])\n",
        "\n",
        "# Run scaled dot-product attention\n",
        "attention_weights, output_matrix = scaled_dot_product_attention(Q, K, V)\n",
        "\n",
        "# Print results\n",
        "print(\"Attention Weights:\\n\", attention_weights)\n",
        "print(\"Final Output:\\n\", output_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ej7Io-EvC_k",
        "outputId": "baa923d6-cc96-4b17-f5f0-7e191a3ec860"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Weights:\n",
            " [[0.73105858 0.26894142]\n",
            " [0.26894142 0.73105858]]\n",
            "Final Output:\n",
            " [[2.07576569 3.07576569 4.07576569 5.07576569]\n",
            " [3.92423431 4.92423431 5.92423431 6.92423431]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" **1. Why Divide the Attention Score by √d?**\n",
        "In **scaled dot-product attention**, the attention score is computed as the dot product of **Query (Q)** and **Key (K)**. Since dot products tend to produce large values when working with high-dimensional vectors, dividing by **√d** (where `d` is the key dimension) helps to:\n",
        "- **Stabilize gradients** – Without scaling, large dot products can cause the softmax function to produce very small probabilities, leading to vanishing gradients.\n",
        "- **Prevent extreme softmax outputs** – Without scaling, softmax might assign overly dominant attention weights, reducing the model's ability to focus on multiple relevant tokens.\n",
        "This ensures the model maintains balanced attention across different words.\n",
        "\n",
        "**2. How Does Self-Attention Help Understand Word Relationships?**\n",
        "Self-attention enables models to **capture contextual dependencies** between words within a sentence by:\n",
        "- **Assigning attention scores to all words** – Instead of looking at words sequentially, self-attention weighs relationships between all tokens in the input, considering long-range dependencies.\n",
        "- **Handling polysemy (multiple meanings)** – For example, in \"The bank on the river\" vs. \"She went to the bank,\" self-attention helps determine whether \"bank\" refers to a financial institution or a riverbank based on surrounding words.\n",
        "- **Enhancing word interactions** – In complex sentences, self-attention allows words to form deeper connections, making it possible for transformers like BERT and GPT to understand meaning effectively.\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "SkivoKgNxBbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q5: Sentiment Analysis using HuggingFace Transformers\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load the pre-trained sentiment analysis model\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "# Input sentence\n",
        "sentence = \"Despite the high price, the performance of the new MacBook is outstanding.\"\n",
        "\n",
        "# Analyze sentiment\n",
        "result = sentiment_pipeline(sentence)[0]\n",
        "\n",
        "# Print results\n",
        "print(f\"Sentiment: {result['label']}\")\n",
        "print(f\"Confidence Score: {result['score']:.4f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2M_mSusmvQ3M",
        "outputId": "ffc63e01-1101-44ad-f2e2-05c322b9daca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment: POSITIVE\n",
            "Confidence Score: 0.9998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" **1. Main Architectural Difference Between BERT and GPT**\n",
        "The key architectural difference lies in their transformer design:\n",
        "- **BERT (Bidirectional Encoder Representations from Transformers)** uses an **encoder-only** architecture. It **processes input in both directions** (left to right and right to left), allowing for deep contextual understanding.\n",
        "- **GPT (Generative Pre-trained Transformer)** is **decoder-based**, meaning it **processes input sequentially (left to right)** and is optimized for **generative tasks**, like text completion and conversational AI.\"\"\"\n",
        "\n",
        "\"\"\" **2. Why Use Pre-Trained Models Like BERT or GPT?**\n",
        "Using pre-trained models instead of training from scratch has several advantages:\n",
        "- **Reduces Computational Cost** – Training large models from scratch requires immense resources, whereas pre-trained models come optimized.\n",
        "- **Leverages Transfer Learning** – Since BERT and GPT are pre-trained on massive corpora, they provide **rich language understanding** without needing large datasets for fine-tuning.\n",
        "- **Faster Development** – Pre-trained models allow for quick adaptation to tasks like sentiment analysis, named entity recognition, and translation with minimal labeled data.\n",
        "- **Better Accuracy** – They learn complex language patterns, improving accuracy even in nuanced tasks like sarcasm detection or intent recognition.\"\"\"\n"
      ],
      "metadata": {
        "id": "1yXF9myHw7Ay"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}